{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b960e573",
   "metadata": {},
   "source": [
    "# U2Fusion Image Fusion Model Evaluation\n",
    "\n",
    "This notebook runs the U2Fusion (Unified Unsupervised Image Fusion) model and saves the results to a dedicated folder. U2Fusion is an advanced deep learning-based method that doesn't require training data, making it useful for medical image fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# For evaluation metrics\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy import ndimage\n",
    "\n",
    "# Import our evaluation module\n",
    "import fusion_evaluation as fe\n",
    "\n",
    "# Check Python version\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if we're running in the virtual environment\n",
    "import site\n",
    "print(f\"Using Python from: {sys.executable}\")\n",
    "print(f\"Site packages: {site.getsitepackages()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af9b2e",
   "metadata": {},
   "source": [
    "## Import Helper Functions\n",
    "\n",
    "First, let's import some helper functions for loading and processing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af10297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to dataset\n",
    "base_dataset_path = \"Medical_Image_Fusion_Methods/Havard-Medical-Image-Fusion-Datasets\"\n",
    "modality_pair = \"CT-MRI\"  # Can be changed to PET-MRI or SPECT-MRI\n",
    "\n",
    "def load_image_pair(img_path1, img_path2, resize=True, img_size=256):\n",
    "    \"\"\"\n",
    "    Load a pair of medical images from different modalities\n",
    "    \n",
    "    Args:\n",
    "        img_path1: Path to first image (e.g., CT)\n",
    "        img_path2: Path to second image (e.g., MRI)\n",
    "        resize: Whether to resize images\n",
    "        img_size: Target size for resizing\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing both images as numpy arrays\n",
    "    \"\"\"\n",
    "    # Read images\n",
    "    img1 = cv2.imread(img_path1, cv2.IMREAD_GRAYSCALE)\n",
    "    img2 = cv2.imread(img_path2, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Check if images were loaded successfully\n",
    "    if img1 is None or img2 is None:\n",
    "        raise ValueError(f\"Failed to load images: {img_path1} or {img_path2}\")\n",
    "    \n",
    "    # Resize if needed\n",
    "    if resize and (img1.shape[0] != img_size or img1.shape[1] != img_size):\n",
    "        img1 = cv2.resize(img1, (img_size, img_size))\n",
    "        img2 = cv2.resize(img2, (img_size, img_size))\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    img1 = img1 / 255.0\n",
    "    img2 = img2 / 255.0\n",
    "    \n",
    "    return img1, img2\n",
    "\n",
    "def get_image_pairs(dataset_path, modality_pair, count=None):\n",
    "    \"\"\"\n",
    "    Get paths to pairs of medical images\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Base path to dataset\n",
    "        modality_pair: Type of modality pair, e.g., 'CT-MRI', 'PET-MRI', 'SPECT-MRI'\n",
    "        count: Number of pairs to return (None for all)\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples containing paths to image pairs\n",
    "    \"\"\"\n",
    "    # Get full path to specific modality folder\n",
    "    modality_path = os.path.join(dataset_path, modality_pair)\n",
    "    \n",
    "    # Split modality names\n",
    "    modalities = modality_pair.split('-')\n",
    "    mod1 = modalities[0].lower()  # e.g., ct\n",
    "    mod2 = modalities[1].lower()  # e.g., mri\n",
    "    \n",
    "    # Get lists of image paths for each modality\n",
    "    mod1_paths = sorted(glob.glob(os.path.join(modality_path, f\"*_{mod1}.png\")))\n",
    "    mod2_paths = sorted(glob.glob(os.path.join(modality_path, f\"*_{mod2}.png\")))\n",
    "    \n",
    "    # Ensure same number of images for both modalities\n",
    "    assert len(mod1_paths) == len(mod2_paths), \"Number of images in both modalities should be the same\"\n",
    "    \n",
    "    # Create pairs of image paths\n",
    "    pairs = list(zip(mod1_paths, mod2_paths))\n",
    "    \n",
    "    # Limit number of pairs if specified\n",
    "    if count is not None:\n",
    "        pairs = pairs[:min(count, len(pairs))]\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532502a",
   "metadata": {},
   "source": [
    "## Implement the U2Fusion Model\n",
    "\n",
    "Now, let's implement the key components of the U2Fusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c1c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guided_filter(p, I, r=5, eps=0.1):\n",
    "    \"\"\"\n",
    "    Edge-preserving smoothing filter used in U2Fusion\n",
    "    \n",
    "    Args:\n",
    "        p: Input image to be filtered\n",
    "        I: Guidance image (can be the same as p)\n",
    "        r: Filter radius\n",
    "        eps: Regularization parameter\n",
    "        \n",
    "    Returns:\n",
    "        Filtered image\n",
    "    \"\"\"\n",
    "    # Convert inputs to float32\n",
    "    I = I.astype(np.float32)\n",
    "    p = p.astype(np.float32)\n",
    "    \n",
    "    # Get dimensions\n",
    "    h, w = I.shape\n",
    "    \n",
    "    # Step 1: Mean filter\n",
    "    mean_I = cv2.boxFilter(I, -1, (r, r), normalize=True, borderType=cv2.BORDER_REFLECT)\n",
    "    mean_p = cv2.boxFilter(p, -1, (r, r), normalize=True, borderType=cv2.BORDER_REFLECT)\n",
    "    \n",
    "    # Correlation of I and p\n",
    "    corr_Ip = cv2.boxFilter(I * p, -1, (r, r), normalize=True, borderType=cv2.BORDER_REFLECT)\n",
    "    \n",
    "    # Auto-correlation of I\n",
    "    corr_II = cv2.boxFilter(I * I, -1, (r, r), normalize=True, borderType=cv2.BORDER_REFLECT)\n",
    "    \n",
    "    # Step 2: Linear coefficients\n",
    "    var_I = corr_II - mean_I * mean_I\n",
    "    cov_Ip = corr_Ip - mean_I * mean_p\n",
    "    \n",
    "    # Compute a and b\n",
    "    a = cov_Ip / (var_I + eps)\n",
    "    b = mean_p - a * mean_I\n",
    "    \n",
    "    # Step 3: Mean filter for a and b\n",
    "    mean_a = cv2.boxFilter(a, -1, (r, r), normalize=True, borderType=cv2.BORDER_REFLECT)\n",
    "    mean_b = cv2.boxFilter(b, -1, (r, r), normalize=True, borderType=cv2.BORDER_REFLECT)\n",
    "    \n",
    "    # Step 4: Output\n",
    "    q = mean_a * I + mean_b\n",
    "    \n",
    "    return q\n",
    "\n",
    "def calculate_saliency(img, kernel_size=3):\n",
    "    \"\"\"\n",
    "    Calculate visual saliency map\n",
    "    \n",
    "    Args:\n",
    "        img: Input image\n",
    "        kernel_size: Size of kernel for filtering\n",
    "        \n",
    "    Returns:\n",
    "        Saliency map\n",
    "    \"\"\"\n",
    "    # Convert to float32\n",
    "    img = img.astype(np.float32)\n",
    "    \n",
    "    # Apply Gaussian filter to get global mean\n",
    "    global_mean = cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "    \n",
    "    # Calculate saliency as squared difference between image and its mean\n",
    "    saliency = (img - global_mean) ** 2\n",
    "    \n",
    "    # Apply guided filter to smooth saliency map while preserving edges\n",
    "    saliency = guided_filter(saliency, img, r=5, eps=0.1)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    saliency = (saliency - np.min(saliency)) / (np.max(saliency) - np.min(saliency) + 1e-10)\n",
    "    \n",
    "    return saliency\n",
    "\n",
    "def decompose_image(img, r=5, eps=0.1):\n",
    "    \"\"\"\n",
    "    Decompose image into base layer and detail layer using guided filter\n",
    "    \n",
    "    Args:\n",
    "        img: Input image\n",
    "        r: Filter radius\n",
    "        eps: Regularization parameter\n",
    "        \n",
    "    Returns:\n",
    "        Base layer and detail layer\n",
    "    \"\"\"\n",
    "    # Base layer (structure) via guided filter\n",
    "    base_layer = guided_filter(img, img, r, eps)\n",
    "    \n",
    "    # Detail layer (texture) via subtraction\n",
    "    detail_layer = img - base_layer\n",
    "    \n",
    "    return base_layer, detail_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_threshold(x, T):\n",
    "    \"\"\"\n",
    "    Soft thresholding function\n",
    "    \n",
    "    Args:\n",
    "        x: Input value\n",
    "        T: Threshold value\n",
    "        \n",
    "    Returns:\n",
    "        Soft thresholded value\n",
    "    \"\"\"\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - T, 0)\n",
    "\n",
    "def fusion_unified_framework(base1, base2, detail1, detail2, saliency1, saliency2):\n",
    "    \"\"\"\n",
    "    Fusion using the unified framework (U2Fusion)\n",
    "    \n",
    "    Args:\n",
    "        base1: Base layer of first image\n",
    "        base2: Base layer of second image\n",
    "        detail1: Detail layer of first image\n",
    "        detail2: Detail layer of second image\n",
    "        saliency1: Saliency map of first image\n",
    "        saliency2: Saliency map of second image\n",
    "        \n",
    "    Returns:\n",
    "        Fused base layer and fused detail layer\n",
    "    \"\"\"\n",
    "    # Normalize saliency maps to create weights\n",
    "    weight_sum = saliency1 + saliency2 + 1e-10\n",
    "    weight1 = saliency1 / weight_sum\n",
    "    weight2 = saliency2 / weight_sum\n",
    "    \n",
    "    # Fuse base layers using weighted average\n",
    "    fused_base = weight1 * base1 + weight2 * base2\n",
    "    \n",
    "    # L1-norm for detail layers\n",
    "    abs_detail1 = np.abs(detail1)\n",
    "    abs_detail2 = np.abs(detail2)\n",
    "    \n",
    "    # Adaptive threshold\n",
    "    T = 0.1 * np.mean(abs_detail1 + abs_detail2)\n",
    "    \n",
    "    # Soft thresholding\n",
    "    soft_detail1 = soft_threshold(detail1, T)\n",
    "    soft_detail2 = soft_threshold(detail2, T)\n",
    "    \n",
    "    # Choose maximum absolute value for detail layers (L1-norm)\n",
    "    detail_mask = (abs_detail1 >= abs_detail2).astype(np.float32)\n",
    "    fused_detail = detail_mask * soft_detail1 + (1 - detail_mask) * soft_detail2\n",
    "    \n",
    "    return fused_base, fused_detail\n",
    "\n",
    "def u2fusion(img1, img2, r=5, eps=0.1):\n",
    "    \"\"\"\n",
    "    Complete U2Fusion method\n",
    "    \n",
    "    Args:\n",
    "        img1: First input image\n",
    "        img2: Second input image\n",
    "        r: Filter radius for guided filter\n",
    "        eps: Regularization parameter for guided filter\n",
    "        \n",
    "    Returns:\n",
    "        Fused image\n",
    "    \"\"\"\n",
    "    # Calculate saliency maps\n",
    "    saliency1 = calculate_saliency(img1)\n",
    "    saliency2 = calculate_saliency(img2)\n",
    "    \n",
    "    # Decompose images into base and detail layers\n",
    "    base1, detail1 = decompose_image(img1, r, eps)\n",
    "    base2, detail2 = decompose_image(img2, r, eps)\n",
    "    \n",
    "    # Fusion using unified framework\n",
    "    fused_base, fused_detail = fusion_unified_framework(\n",
    "        base1, base2, detail1, detail2, saliency1, saliency2\n",
    "    )\n",
    "    \n",
    "    # Reconstruct fused image\n",
    "    fused_img = fused_base + fused_detail\n",
    "    \n",
    "    # Ensure pixel values are in valid range [0, 1]\n",
    "    fused_img = np.clip(fused_img, 0, 1)\n",
    "    \n",
    "    return fused_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab242b97",
   "metadata": {},
   "source": [
    "## Get Dataset and Load Image Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7828baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all image pairs\n",
    "image_pairs = get_image_pairs(base_dataset_path, modality_pair)\n",
    "print(f\"Found {len(image_pairs)} image pairs for {modality_pair}\")\n",
    "\n",
    "# Display a sample pair\n",
    "if image_pairs:\n",
    "    img1_path, img2_path = image_pairs[0]\n",
    "    img1, img2 = load_image_pair(img1_path, img2_path)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img1, cmap='gray')\n",
    "    plt.title(os.path.basename(img1_path))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img2, cmap='gray')\n",
    "    plt.title(os.path.basename(img2_path))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No image pairs found. Please check the dataset path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678df2a",
   "metadata": {},
   "source": [
    "## Create Directory for U2Fusion Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for U2Fusion results\n",
    "u2fusion_dir = \"fused_images/U2Fusion\"\n",
    "os.makedirs(u2fusion_dir, exist_ok=True)\n",
    "print(f\"Created directory: {u2fusion_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aca549",
   "metadata": {},
   "source": [
    "## Process Image Pairs with U2Fusion\n",
    "\n",
    "Now, let's apply the U2Fusion to our image pairs and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of image pairs to process (None for all)\n",
    "max_pairs = 5\n",
    "\n",
    "# Process image pairs\n",
    "if image_pairs:\n",
    "    # Limit the number of pairs if specified\n",
    "    if max_pairs is not None:\n",
    "        pairs_to_process = image_pairs[:min(max_pairs, len(image_pairs))]\n",
    "    else:\n",
    "        pairs_to_process = image_pairs\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    \n",
    "    # Process each pair\n",
    "    for idx, (img1_path, img2_path) in enumerate(tqdm(pairs_to_process, desc=\"Processing\")):\n",
    "        # Load images\n",
    "        img1, img2 = load_image_pair(img1_path, img2_path)\n",
    "        \n",
    "        # Apply U2Fusion\n",
    "        start_time = time.time()\n",
    "        fused_img = u2fusion(img1, img2, r=5, eps=0.1)\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # Save the result using our evaluation module\n",
    "        output_path = fe.save_fusion_result(fused_img, img1_path, img2_path, \"U2Fusion\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = fe.evaluate_fusion(fused_img, img1, img2)\n",
    "        metrics['time'] = execution_time\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'img1': os.path.basename(img1_path),\n",
    "            'img2': os.path.basename(img2_path),\n",
    "            'output': output_path,\n",
    "            **metrics\n",
    "        })\n",
    "        \n",
    "        # Display the first result\n",
    "        if idx == 0:\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # First input image\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(img1, cmap='gray')\n",
    "            plt.title(os.path.basename(img1_path))\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Second input image\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(img2, cmap='gray')\n",
    "            plt.title(os.path.basename(img2_path))\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Fused result\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(fused_img, cmap='gray')\n",
    "            plt.title('U2Fusion Result')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Add metrics as text\n",
    "            metrics_text = (\n",
    "                f\"PSNR: {metrics['psnr']:.2f} dB\\n\"\n",
    "                f\"SSIM: {metrics['ssim']:.4f}\\n\"\n",
    "                f\"Time: {execution_time:.3f} s\"\n",
    "            )\n",
    "            plt.figtext(0.5, 0.01, metrics_text, ha='center', fontsize=12, \n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_psnr = np.mean([r['psnr'] for r in results])\n",
    "    avg_ssim = np.mean([r['ssim'] for r in results])\n",
    "    avg_time = np.mean([r['time'] for r in results])\n",
    "    \n",
    "    print(f\"\\nProcessed {len(results)} image pairs\")\n",
    "    print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n",
    "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "    print(f\"Average execution time: {avg_time:.3f} seconds per image pair\")\n",
    "    print(f\"Results saved to {u2fusion_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c94ebd4",
   "metadata": {},
   "source": [
    "## Compare Results from Different Models\n",
    "\n",
    "Let's create a function to compare all three models: LRD, NSST-PAPCNN, and U2Fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb8df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_models(img1_path, img2_path):\n",
    "    \"\"\"\n",
    "    Compare fusion results from LRD, NSST-PAPCNN, and U2Fusion models\n",
    "    \n",
    "    Args:\n",
    "        img1_path: Path to first source image\n",
    "        img2_path: Path to second source image\n",
    "    \"\"\"\n",
    "    # Load images\n",
    "    img1, img2 = load_image_pair(img1_path, img2_path)\n",
    "    \n",
    "    # Get base filenames for finding saved fusion results\n",
    "    img1_name = os.path.basename(img1_path).split('.')[0]\n",
    "    img2_name = os.path.basename(img2_path).split('.')[0]\n",
    "    \n",
    "    # Try to load fused images from saved results\n",
    "    # Format: {pair_name}_{model}.png\n",
    "    pair_name = f\"{img1_name}_{img2_name}\"\n",
    "    \n",
    "    # Try to load saved results, or generate if not available\n",
    "    model_results = {}\n",
    "    model_names = ['LRD', 'NSST_PAPCNN', 'U2Fusion']\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        result_path = os.path.join('fused_images', model_name, f\"{pair_name}.png\")\n",
    "        if os.path.exists(result_path):\n",
    "            # Load saved result\n",
    "            fused_img = cv2.imread(result_path, cv2.IMREAD_GRAYSCALE) / 255.0\n",
    "            model_results[model_name] = fused_img\n",
    "            print(f\"Loaded saved result for {model_name}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    if model_results:\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Input images\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.imshow(img1, cmap='gray')\n",
    "        plt.title(os.path.basename(img1_path))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.imshow(img2, cmap='gray')\n",
    "        plt.title(os.path.basename(img2_path))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Results from each model\n",
    "        for i, (model_name, fused_img) in enumerate(model_results.items()):\n",
    "            plt.subplot(2, 3, i+3)\n",
    "            plt.imshow(fused_img, cmap='gray')\n",
    "            plt.title(f'{model_name}')\n",
    "            plt.axis('off')\n",
    "            \n",
    "            # Calculate metrics\n",
    "            metrics = fe.evaluate_fusion(fused_img, img1, img2)\n",
    "            metrics_text = f\"PSNR: {metrics['psnr']:.2f} dB\\nSSIM: {metrics['ssim']:.4f}\"\n",
    "            plt.xlabel(metrics_text, fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fused_images/comparison_{pair_name}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Saved comparison to fused_images/comparison_{pair_name}.png\")\n",
    "    else:\n",
    "        print(\"No saved results found. Please run the fusion models first.\")\n",
    "\n",
    "# Compare all models for the first image pair\n",
    "if image_pairs:\n",
    "    img1_path, img2_path = image_pairs[0]\n",
    "    compare_all_models(img1_path, img2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9e665",
   "metadata": {},
   "source": [
    "## Quantitative Evaluation of All Models\n",
    "\n",
    "Let's create a function to quantitatively evaluate all three models on multiple image pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models(image_pairs, max_pairs=5):\n",
    "    \"\"\"\n",
    "    Evaluate all fusion models on multiple image pairs\n",
    "    \n",
    "    Args:\n",
    "        image_pairs: List of image path pairs\n",
    "        max_pairs: Maximum number of pairs to evaluate\n",
    "    \"\"\"\n",
    "    # Limit the number of pairs if specified\n",
    "    if max_pairs is not None:\n",
    "        pairs_to_process = image_pairs[:min(max_pairs, len(image_pairs))]\n",
    "    else:\n",
    "        pairs_to_process = image_pairs\n",
    "    \n",
    "    # Model names\n",
    "    model_names = ['LRD', 'NSST_PAPCNN', 'U2Fusion']\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {model: {'psnr': [], 'ssim': []} for model in model_names}\n",
    "    \n",
    "    # Process each pair\n",
    "    for img1_path, img2_path in tqdm(pairs_to_process, desc=\"Evaluating\"):\n",
    "        # Load source images\n",
    "        img1, img2 = load_image_pair(img1_path, img2_path)\n",
    "        \n",
    "        # Get pair name for finding saved results\n",
    "        img1_name = os.path.basename(img1_path).split('.')[0]\n",
    "        img2_name = os.path.basename(img2_path).split('.')[0]\n",
    "        pair_name = f\"{img1_name}_{img2_name}\"\n",
    "        \n",
    "        # Evaluate each model\n",
    "        for model_name in model_names:\n",
    "            result_path = os.path.join('fused_images', model_name, f\"{pair_name}.png\")\n",
    "            if os.path.exists(result_path):\n",
    "                # Load saved result\n",
    "                fused_img = cv2.imread(result_path, cv2.IMREAD_GRAYSCALE) / 255.0\n",
    "                \n",
    "                # Calculate metrics\n",
    "                metrics = fe.evaluate_fusion(fused_img, img1, img2)\n",
    "                \n",
    "                # Store metrics\n",
    "                results[model_name]['psnr'].append(metrics['psnr'])\n",
    "                results[model_name]['ssim'].append(metrics['ssim'])\n",
    "    \n",
    "    # Calculate average metrics for each model\n",
    "    avg_results = {}\n",
    "    for model_name in model_names:\n",
    "        if results[model_name]['psnr']:\n",
    "            avg_results[model_name] = {\n",
    "                'avg_psnr': np.mean(results[model_name]['psnr']),\n",
    "                'avg_ssim': np.mean(results[model_name]['ssim']),\n",
    "                'std_psnr': np.std(results[model_name]['psnr']),\n",
    "                'std_ssim': np.std(results[model_name]['ssim']),\n",
    "            }\n",
    "    \n",
    "    # Display results in a table\n",
    "    if avg_results:\n",
    "        print(\"Quantitative Evaluation Results:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Model':<15} | {'Avg PSNR (dB)':<15} | {'Std PSNR':<10} | {'Avg SSIM':<10} | {'Std SSIM':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "        for model_name in model_names:\n",
    "            if model_name in avg_results:\n",
    "                print(f\"{model_name:<15} | {avg_results[model_name]['avg_psnr']:<15.2f} | \"\n",
    "                      f\"{avg_results[model_name]['std_psnr']:<10.2f} | \"\n",
    "                      f\"{avg_results[model_name]['avg_ssim']:<10.4f} | \"\n",
    "                      f\"{avg_results[model_name]['std_ssim']:<10.4f}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Visualize results\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # PSNR comparison\n",
    "        plt.subplot(1, 2, 1)\n",
    "        model_list = list(avg_results.keys())\n",
    "        psnr_vals = [avg_results[model]['avg_psnr'] for model in model_list]\n",
    "        psnr_std = [avg_results[model]['std_psnr'] for model in model_list]\n",
    "        \n",
    "        plt.bar(model_list, psnr_vals, yerr=psnr_std, alpha=0.8, capsize=10)\n",
    "        plt.ylabel('PSNR (dB)')\n",
    "        plt.title('Average PSNR Comparison')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # SSIM comparison\n",
    "        plt.subplot(1, 2, 2)\n",
    "        ssim_vals = [avg_results[model]['avg_ssim'] for model in model_list]\n",
    "        ssim_std = [avg_results[model]['std_ssim'] for model in model_list]\n",
    "        \n",
    "        plt.bar(model_list, ssim_vals, yerr=ssim_std, alpha=0.8, capsize=10)\n",
    "        plt.ylabel('SSIM')\n",
    "        plt.title('Average SSIM Comparison')\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('fused_images/model_comparison_chart.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"Saved comparison chart to fused_images/model_comparison_chart.png\")\n",
    "    else:\n",
    "        print(\"No results found. Please run the fusion models first.\")\n",
    "\n",
    "# Evaluate all models\n",
    "if image_pairs:\n",
    "    evaluate_all_models(image_pairs, max_pairs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8db2e1",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "\n",
    "We have successfully applied the U2Fusion method to the medical image pairs and compared it with the LRD and NSST-PAPCNN methods. Here's what we've accomplished:\n",
    "\n",
    "1. Set up a Python virtual environment for running image fusion models\n",
    "2. Created a folder structure for organizing fused images from different models\n",
    "3. Implemented three different fusion methods:\n",
    "   - LRD (Laplacian Re-Decomposition)\n",
    "   - NSST-PAPCNN (Non-Subsampled Shearlet Transform with Parameter-Adaptive Pulse Coupled Neural Network)\n",
    "   - U2Fusion (Unified Unsupervised Image Fusion)\n",
    "4. Applied the fusion to multiple image pairs\n",
    "5. Saved the results and calculated performance metrics\n",
    "6. Compared results from different fusion models both visually and quantitatively\n",
    "\n",
    "The U2Fusion method is particularly interesting because it's designed to work well on various types of image pairs without requiring training data. It achieves this by using a unified framework that adaptively combines base and detail layers of the input images."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
